{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You are ready to learn and do your best.', 'but you are also nervous.']\n",
      "['You', 'are', 'ready', 'to', 'learn', 'and', 'do', 'your', 'best', '.', 'but', 'you', 'are', 'also', 'nervous', '.']\n",
      "[['You', 'are', 'ready', 'to', 'learn', 'and', 'do', 'your', 'best', '.'], ['but', 'you', 'are', 'also', 'nervous', '.']]\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"You are ready to learn and do your best. but you are also nervous.\"\n",
    "sents = (sent_tokenize(text)) \n",
    "print sents\n",
    "print(word_tokenize(text))\n",
    "words = [word_tokenize(sent) for sent in sents]\n",
    "print words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'ready', 'learn', 'best', 'also', 'nervous']\n"
     ]
    }
   ],
   "source": [
    "#Removing Stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "text = \"You are ready to learn and do your best. but you are also nervous.\"\n",
    "#make set of stopword and punctuation\n",
    "customstopwords=set(stopwords.words('english')+list(punctuation))\n",
    "wordslist=[word for word in word_tokenize(text) if word not in customstopwords]\n",
    "print wordslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('You', 'ready'), 1),\n",
       " (('also', 'nervous'), 1),\n",
       " (('best', 'also'), 1),\n",
       " (('learn', 'best'), 1),\n",
       " (('ready', 'learn'), 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N-grams\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "#trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wordslist)\n",
    "#most important bigram on top\n",
    "sorted(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'import', 'to', 'by', 'very', 'python', 'whil', 'you', 'ar', 'python', 'with', 'python', '.', 'al', 'python', 'hav', 'python', 'poor', 'at', 'least', u'ont', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "stemmedwords=[st.stem(word) for word in word_tokenize(new_text)]\n",
    "print stemmedwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('important', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('by', 'IN'),\n",
       " ('very', 'RB'),\n",
       " ('pythonly', 'RB'),\n",
       " ('while', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('pythoning', 'VBG'),\n",
       " ('with', 'IN'),\n",
       " ('python', 'NN'),\n",
       " ('.', '.'),\n",
       " ('All', 'DT'),\n",
       " ('pythoners', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('pythoned', 'VBN'),\n",
       " ('poorly', 'RB'),\n",
       " ('at', 'IN'),\n",
       " ('least', 'JJS'),\n",
       " ('once', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part of Speech\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(word_tokenize(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Synset('mouse.n.01'), u'any of numerous small rodents typically resembling diminutive rats having pointed snouts and small ears on elongated bodies with slender usually hairless tails')\n",
      "(Synset('shiner.n.01'), u'a swollen bruise caused by a blow to the eye')\n",
      "(Synset('mouse.n.03'), u'person who is quiet or timid')\n",
      "(Synset('mouse.n.04'), u'a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad')\n",
      "(Synset('sneak.v.01'), u'to go stealthily or furtively')\n",
      "(Synset('mouse.v.02'), u'manipulate the mouse of a computer')\n"
     ]
    }
   ],
   "source": [
    "#Word Sense Disambiguation\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "#nltk.download('wordnet')\n",
    "for ss in wn.synsets('mouse'):\n",
    "    print (ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Synset('bass.n.07'), u'the member with the lowest range of a family of musical instruments')\n",
      "(Synset('sea_bass.n.01'), u'the lean flesh of a saltwater fish of the family Serranidae')\n",
      "(Synset('mouse.v.02'), u'manipulate the mouse of a computer')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sense1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"), 'bass')\n",
    "print (sense1, sense1.definition())\n",
    "\n",
    "sense2 = lesk(word_tokenize(\"The sea bass really very hard to catch\"), 'bass')\n",
    "print (sense2, sense2.definition())\n",
    "\n",
    "sense3 = lesk(word_tokenize(\"Cat is chasing mouse\"), 'mouse')\n",
    "print (sense3, sense3.definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spam Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
